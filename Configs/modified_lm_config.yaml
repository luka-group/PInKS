lm_module:
  model_name_or_path: 'distilbert-base-cased'
#  model_name_or_path: 'distilbert-base-uncased'
  learning_rate: 5e-5
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8

trainer_args:
  accumulate_grad_batches: 4
  limit_train_batches: 1.0


hardware:
  gpus: "2"

data_module:
  train_file: /nas/home/${username}/CQplus/Outputs/process_dataset_using_snorkel/0.7/filtered_dataset.csv
  train_batch_size: 2 #4

  validation_file:
  val_batch_size: 8

  line_by_line: true
  pad_to_max_length: false
  preprocessing_num_workers: 4
  overwrite_cache: false
  max_seq_length: 256
  mlm_probability: 0.15 # It is not important

  dataloader_num_workers: 1 #4

  datasetloader_kwargs:
      skiprows: 0
  #    column_names: ["line"]


ray:
  num_samples: 10
#  gpus_per_trial: 1
  sweep_dict:
   "lm_module.learning_rate": "tune.loguniform(1e-6, 1e-4)"
   "data_module.train_batch_size": "tune.choice([2, 4])"
   "data_module.max_seq_length": "tune.choice([200, 300, 400, 450])"


username: "qasemi"

hydra:
  run:
    dir: "/nas/home/${username}/CQplus/Outputs/${hydra.job.name}"
