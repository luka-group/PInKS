#weak_cq_path: "/nas/home/qasemi/CQplus/Outputs/RemoveSimpleWeakPNLI/weakcq_filtered.csv"
weak_cq_path: "/nas/home/qasemi/CQplus/Outputs/process_dataset_using_snorkel/0.0/filtered_dataset.csv"

#cq_path: "/nas/home/qasemi/Mowgli-CoreQuisite/outputs/EvaluateBatch/MCQ-2000/BasicBenchmark/test.csv"
cq_path: "/nas/home/qasemi/CQplus/Outputs/RemoveSimplePNLI/test_filtered.csv"
mnli_path: "/nas/home/qasemi/CQplus/Outputs/Corpora/MNLI/multinli_1.0/multinli_1.0_train.jsonl"

dnli_path: "/nas/home/qasemi/CQplus/Outputs/Other_NLI/dnli_nli_test.csv"
atomic_nli_path: "/nas/home/qasemi/CQplus/Outputs/Other_NLI/atomic_nli.csv"

model_setup:
  model_name: ""
  tuned_model_path:
  max_length: ${data_module.max_seq_length}

hardware:
  gpus: '2'
  cpu_limit: 16
#  distributed_backend: #'dp' # None

train_setup:
  do_train: false
  accumulate_grad_batches: 4

  max_epochs: 1
  limit_train_batches: 1.0
  val_check_interval: 0.1
  warmup_steps: 5

  learning_rate: 4e-06
  adam_epsilon: 1e-08
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.998

  batch_size: ${data_module.train_batch_size}

data_module:
  train_batch_size: 2 #4
  val_batch_size: 4
  preprocessing_batch_size: 100

  pad_to_max_length: false
  preprocessing_num_workers: 4
  overwrite_cache: false

  dataloader_num_workers: 4

  max_seq_length: 400

  train_composition: ['weakcq', 'dnli', 'mnli', 'cq', 'atomic']
  test_composition: ['cq']

  train_strategy: "continual" # "multitask"

  use_class_weights: false

hydra:
  run:
    dir: /nas/home/qasemi/CQplus/Outputs/${hydra.job.name}


ray:
  num_samples: 10
#  gpus_per_trial: 1
  sweep_dict:
#    "train_setup.learning_rate": "tune.loguniform(1e-6, 1e-4)"
#    "data_module.train_batch_size": "tune.choice([2, 4])"
#    "data_module.max_seq_length": "tune.choice([200, 300, 400, 450])"
# Comparitibility configs
lm_module:
  model_name_or_path: ${model_setup.model_name}
  learning_rate: ${train_setup.learning_rate}
  adam_epsilon: ${train_setup.adam_epsilon}
  weight_decay: ${train_setup.weight_decay}
  adam_beta1: ${train_setup.beta1}
  adam_beta2: ${train_setup.beta2}

trainer_args:
  accumulate_grad_batches: ${train_setup.accumulate_grad_batches}
  limit_train_batches: ${train_setup.limit_train_batches}
