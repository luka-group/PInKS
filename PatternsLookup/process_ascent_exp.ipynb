{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import Dict, Generator\n",
    "\n",
    "import pandas as pd\n",
    "import IPython\n",
    "import hydra\n",
    "import omegaconf\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from Patterns import PatternUtils\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/nas/home/pkhanna/CQplus/Outputs/snorkel_process_omcs_using_snorkel/snorkel_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                          We can wonder whether if it will rain\n",
       "1                                                                                        Your feet might come into contact with something if it is on the floor.\n",
       "2                                       You might trip if your foot comes into contact with something and you do not have enough time to react to the situation.\n",
       "3                                                                                            You would pay by check because you want to pay but don't have cash.\n",
       "4    The statement \"A person can look underneath things if he wants to find a missing key.\" helps answer the question \"What can a person do find a missing key?\"\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_assertions(concept: Dict) -> Generator[Dict, None, None]:\n",
    "    assert_keys = ['general_assertions', 'subgroup_assertions', 'aspect_assertions']\n",
    "    for k in assert_keys:\n",
    "        for asrt in concept[k]:\n",
    "            for c in asrt['clusters']:\n",
    "                for f in c['facets']:\n",
    "                    yield {\n",
    "                        'subject': c['subject'],\n",
    "                        'predicate': c['predicate'],\n",
    "                        'object': c['object'],\n",
    "                        'facet_value': f[\"value\"],\n",
    "                        'facet_label': f[\"label\"],\n",
    "                    }\n",
    "\n",
    "\n",
    "def extract_usedfor_assertions(config: omegaconf.dictconfig.DictConfig):\n",
    "    logger.info(f'loading json from {config.ascent_path}')\n",
    "    output = []\n",
    "\n",
    "    pbar_concept = tqdm(desc='concepts')\n",
    "    pbar_assert = tqdm(desc=f'\\\"{config.predicate}\\\" assertions')\n",
    "\n",
    "    for df_chunk in pd.read_json(config.ascent_path, lines=True,  chunksize=100):\n",
    "        for i, concept in df_chunk.iterrows():\n",
    "            pbar_concept.update()\n",
    "            for asrt in _iter_assertions(concept.to_dict()):\n",
    "                if config.predicate in asrt['predicate']:\n",
    "                    output.append(asrt)\n",
    "                    pbar_assert.update()\n",
    "\n",
    "    logger.info(f'converting to pandas')\n",
    "    df = pd.DataFrame(output)\n",
    "    df.to_csv(config.output_names.extract_usedfor_assertions)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_usedfor_sentences(config: omegaconf.dictconfig.DictConfig):\n",
    "    logger.info(f'loading json from {config.ascent_path}')\n",
    "    output = []\n",
    "\n",
    "    pbar_concept = tqdm(desc='concepts')\n",
    "    pbar_assert = tqdm(desc=f'\\\"{config.predicate}\\\" assertions')\n",
    "\n",
    "    for df_chunk in pd.read_json(config.ascent_path, lines=True, chunksize=100):\n",
    "        for i, concept in df_chunk.iterrows():\n",
    "            pbar_concept.update()\n",
    "\n",
    "            # create sources lut\n",
    "            sent_dict = {}\n",
    "            for k, s in concept['sentences'].items():\n",
    "                sent_dict[k] = s['text']\n",
    "\n",
    "            # go through assertions\n",
    "            assert_keys = ['general_assertions', 'subgroup_assertions', 'aspect_assertions']\n",
    "            for k in assert_keys:\n",
    "                for asrt in concept[k]:\n",
    "                    for c in asrt['clusters']:\n",
    "                        if config.predicate not in c['predicate']:\n",
    "                            continue\n",
    "                        if len(c['facets']) == 0:\n",
    "                            continue\n",
    "\n",
    "                        out_dict = {\n",
    "                            'sources': [\n",
    "                                sent_dict[sd['sentence_hash']]\n",
    "                                for sd in c['sources']\n",
    "                            ],\n",
    "                            'facets': []\n",
    "                        }\n",
    "                        # out_dict['facets'] = []\n",
    "                        for f in c['facets']:\n",
    "                            out_dict['facets'].append({\n",
    "                                'subject': c['subject'],\n",
    "                                'predicate': c['predicate'],\n",
    "                                'object': c['object'],\n",
    "                                'facet_value': f[\"value\"],\n",
    "                                'facet_label': f[\"label\"],\n",
    "                            })\n",
    "\n",
    "\n",
    "                        pbar_assert.update()\n",
    "                        output.append(out_dict)\n",
    "\n",
    "    logger.info(f'converting to pandas')\n",
    "    df = pd.DataFrame(output)\n",
    "    df.to_csv(config.output_names.extract_usedfor_sentences)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_all_sentences(config: omegaconf.dictconfig.DictConfig):\n",
    "    print(\"Printing Config Update!!!!!!!\")\n",
    "    print(config.predicate)\n",
    "    # assert config.predicate == '*', f'{config.predicate}'\n",
    "    logger.info(f'loading json from {config.ascent_path}')\n",
    "    output = []\n",
    "\n",
    "    pbar_concept = tqdm(desc='concepts')\n",
    "    pbar_assert = tqdm(desc=f'\\\"{config.predicate}\\\" assertions')\n",
    "\n",
    "    for df_chunk in pd.read_json(config.ascent_path, lines=True, chunksize=100):\n",
    "        for i, concept in df_chunk.iterrows():\n",
    "            pbar_concept.update()\n",
    "\n",
    "            # create sources lut\n",
    "            sent_dict = {}\n",
    "            for k, s in concept['sentences'].items():\n",
    "                sent_dict[k] = s['text'].replace('\\n', ' ')\n",
    "\n",
    "            # go through assertions\n",
    "            assert_keys = ['general_assertions', 'subgroup_assertions', 'aspect_assertions']\n",
    "            for k in assert_keys:\n",
    "                for asrt in concept[k]:\n",
    "                    for c in asrt['clusters']:\n",
    "\n",
    "                        out_dict = {\n",
    "                            'subject': c['subject'].replace('\\n', ' '),\n",
    "                            'predicate': c['predicate'].replace('\\n', ' '),\n",
    "                            'object': c['object'].replace('\\n', ' '),\n",
    "\n",
    "                            'sources': [\n",
    "                                sent_dict[sd['sentence_hash']]\n",
    "                                for sd in c['sources']\n",
    "                            ],\n",
    "                            'facets': []\n",
    "                        }\n",
    "                        for f in c['facets']:\n",
    "                            out_dict['facets'].append({\n",
    "                                'label': f[\"label\"].replace('\\n', ' '),\n",
    "                                'value': f[\"value\"].replace('\\n', ' '),\n",
    "                            })\n",
    "\n",
    "                        pbar_assert.update()\n",
    "                        output.append(out_dict)\n",
    "\n",
    "    logger.info(f'converting to pandas')\n",
    "    df = pd.DataFrame(output)\n",
    "    # df.to_json('all_sentences.json')\n",
    "    df.to_json(config.output_names.extract_all_sentences, orient='records', lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_all_sentences(config: omegaconf.dictconfig.DictConfig):\n",
    "    from Patterns import PatternUtils\n",
    "    all_sents_path = pathlib.Path(os.getcwd())/pathlib.Path(config.output_names.extract_all_sentences)\n",
    "\n",
    "    assert all_sents_path.exists(), all_sents_path\n",
    "\n",
    "    matches = {}\n",
    "    df_matches = []\n",
    "    for p, label in [\n",
    "        [r'{negative_precondition} {ENB_CONJ} {action}\\.', 'CONTRADICT'],\n",
    "        [r'\\. {any_word} unless {precondition}, {action}\\.', 'CONTRADICT'],\n",
    "        [r'{precondition} makes {action} impossible.', 'CONTRADICT'],\n",
    "        [r'{action} unless {precondition}\\.', 'CONTRADICT'],\n",
    "        # [r'{any_word} unless {precondition_action}\\.', 'CONTRADICT'],\n",
    "\n",
    "        [r'{action} only if {precondition}.', 'ENTAILMENT'],\n",
    "        [r'{precondition} {ENB_CONJ} {action}.', 'ENTAILMENT'],\n",
    "        [r'{precondition} makes {action} possible.', 'ENTAILMENT'],\n",
    "    ]:\n",
    "        matches[p] = PatternUtils.check_pattern_in_file_grep(\n",
    "                p, base_path=os.getcwd(), files_pattern=config.output_names.extract_all_sentences,\n",
    "                do_srl=config.process_all_sentences.do_srl, label=label)\n",
    "\n",
    "        p_len = len(matches[p])\n",
    "        logger.warning(f'Found {p_len} hits on {p}')\n",
    "\n",
    "        df_matches.append(pd.DataFrame(matches[p]))\n",
    "\n",
    "        logger.info(f'Dumping match results')\n",
    "        with open(config.output_names.process_all_sentences, 'w') as fp:\n",
    "            json.dump(matches, fp)\n",
    "\n",
    "    logger.info(\"Dumping csv file\")\n",
    "    pd.concat(df_matches, axis=0).to_csv(config.output_names.process_all_sentences.replace('.json', '.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(config_path=\"../Configs\", config_name=\"process_ascent_config\")\n",
    "def main(config: omegaconf.dictconfig.DictConfig):\n",
    "    logger.warning(f'Config: {config}')\n",
    "\n",
    "    if config.method == 'extract_all_sentences':\n",
    "        temp_df=extract_all_sentences(config)\n",
    "    elif config.method == 'process_all_sentences':\n",
    "        process_all_sentences(config)\n",
    "    elif config.method == 'extract_usedfor_assertions':\n",
    "        extract_usedfor_assertions(config)\n",
    "    elif config.method == 'extract_usedfor_sentences':\n",
    "        extract_usedfor_sentences(config)\n",
    "    # pd.read_json(config)\n",
    "    # extract_usedfor_assertions(config)\n",
    "    # extract_usedfor_sentences(config)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
